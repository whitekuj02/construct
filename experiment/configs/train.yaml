
train:
  model:
    #name: "Bllossom/llama-3.2-Korean-Bllossom-3B" #"Bllossom/llama-3.2-Korean-Bllossom-3B" # "NCSOFT/Llama-VARCO-8B-Instruct"
    name : "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B"
    quantization:
      use: True
      config:
        load_in_8bit: True
        # load_in_4bit: True
        # bnb_4bit_use_double_quant: True
        # bnb_4bit_quant_type: "nf4"
        bnb_4bit_compute_dtype: bfloat16

    lora:
      use: True
      config:
        task_type: "CAUSAL_LM"
        r: 32 
        lora_alpha: 8
        lora_dropout: 0.1
        target_modules: ["q_proj", "v_proj", "k_proj"]  # 실제 모델의 모듈 이름에 맞게 설정

  parameter_save: "/home/aicontest/construct/experiment/src/parameter"

  SFT: True
  training:
    output_dir: "./outputs"
    per_device_train_batch_size: 4
    # max_steps: 5000
    # gradient_accumulation_steps: 4
    num_train_epochs: 2 
    save_total_limit: 1
    logging_dir: "./logs"
    logging_steps: 10
    learning_rate: 0.0002
    weight_decay: 0.1
    fp16: True  # mixed precision 사용
    push_to_hub: False
    report_to: "none"


  test:
    dir: "4_prompt_aug_5b"